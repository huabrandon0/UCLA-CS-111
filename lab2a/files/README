NAME: Brandon Hua
EMAIL: huabrandon0@gmail.com
ID: 804595738
README

SLIP DAYS USED: 2

lab2_add.c:
Source code of the lab2_add program, which tests a shared variable add function.

SortedList.h:
Header file describing the interfaces for linked list operations.

SortedList.c:
Implementations of the interfaces described by SortedList.h.

lab2_list.c:
Source code of the lab2_list program, which tests shared list operations.

Makefile:
Builds the lab2_add and lab2_list programs, outputs, graphs, and tarball.
Supported targets include build, tests, graphs, dist, and clean.

lab2_add.csv:
Outputs generated by lab2_add tests.

lab2_list.csv:
Outputs generated by lab2_list tests.

lab2_add-1.png through lab2_add-5.png:
Graphs that measure the performance of the lab2_add program.

lab2_list-1.png through lab2_list-4.png:
Graphs that measure the performance of the lab2_list program.

add_tests.sh, list_tests.sh:
Shell scripts to run test cases and generate the .csv files.

lab2_add.gp, lab2_list.gp:
Data reduction scripts (gnuplot) that produce the .png files.

QUESTION 2.1.1
It takes many iterations before errors are seen because errors only happen
when two threads try to update the counter at the same time, which is a fairly
uncommon occurrence without a high number of iterations.

A significantly smaller number of iterations seldom fails because there are
fewer times that a thread can update the counter. Thus, the chance of two
threads updating the counter at the same time is very low. Also, because
threads are independently scheduled, a smaller number of iterations will
make threads complete faster, and this may decrease the amount of time that
threads run concurrently. So, the chance of race conditions decreases.

QUESTION 2.1.2
The --yield runs are slower because threads are calling sched_yield and
relinquishing the CPU. Each time this happens (most of the time), a new
thread is chosen to run and a context switch must occur.

The additional time is going to the cost of context switching.

It is not possible to get valid per-operation timings while using the --yield
option.

We cannot do this because we cannot separate the time used for operations from
the time used for context switches.

QUESTION 2.1.3
At a low number of iterations, the overhead of creating a thread dominates the
time used. The average cost per operation drops with increasing iterations
because the overhead of creating a thread becomes less significant.

The correct cost per iteration can be found at an infinite number of iterations.
Because it is impossible to run an infinite number of iterations, we can only
estimate the correct cost by running a very large number of iterations, such
that the cost per iteration seems nearly constant.

QUESTION 2.1.4
All options perform similarly for low numbers of threads because there are
fewer times that threads are trying to update the counter at the same time
(entering the critical section). Therefore, threads aren't spending time
waiting for locks to be free and so the synchronization techniques have less
impact on time per operation.

The three protected operations slow down as the number of threads rises because
there are more times that threads are trying to update the counter at the same
time. Therefore, threads are waiting for locks to be free and so the
synchronization techniques have more of an impact on time per operation.

QUESTION 2.2.1
The mutex-protected list operations have costs that consistently rise up
with the number of threads (shape is linearly rising). The mutex-protected add
operation has costs that first rise then fall off (shape is like a hill/mound).
The difference can be attributed to the higher costs of list operations than a
simple add operation. This would make the cost rise steadily for the list
operations, because more time is spent in the critical section.

Note: the shapes I describe are the ones on the graphs generated by gnuplot,
which have axes on logarithmic scales.

QUESTION 2.2.2
Spinlock-protected operations generally have higher cost than mutex-protected
operations. Their costs also rise more drastically vs the number of threads
than their mutex-protected counterparts' costs. Both costs have a rising slope
shape, but the spinlock-protected slope is much more steep. This can be
attributed to the wasted CPU cycles that spinlocks use up while waiting on
locks, as opposed to blocking while waiting on locks which mutexes do.